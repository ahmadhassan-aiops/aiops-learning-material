

A node selector in Kubernetes is a simple way to constrain a pod to run only on nodes that match a specific label. It's part of Kubernetes scheduling, 
allowing you to guide where pods are placed based on node attributes. When you define a node selector in a pod specification, Kubernetes compares the 
selector key-value pair against the labels on each node. Only nodes that match this label will be eligible to run the pod. It is a basic form of node 
affinity‚Äîideal for straightforward scheduling needs where you want to match one or more exact labels.


Suppose you have a Kubernetes cluster with two types of nodes: standard nodes for general workloads and high-memory nodes labeled with type=high-memory 
for memory-intensive applications. You want to run a database pod that requires a large amount of RAM only on the high-memory nodes. 
By labeling the appropriate nodes (kubectl label nodes <node-name> type=high-memory) and setting a node selector in the pod definition 
(nodeSelector: type: high-memory), Kubernetes ensures the database pod is only scheduled onto nodes that can meet its memory requirements, 
preventing it from running on unsuitable nodes.


-----------------------------------------------

cat kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker




kind create cluster --config kind-config.yaml
Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.32.2) üñº
 ‚úì Preparing nodes üì¶ üì¶ üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
 ‚úì Joining worker nodes üöú
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ





[root@client1 ahmad]# kubectl get nodes
NAME                 STATUS     ROLES           AGE   VERSION
kind-control-plane   Ready      control-plane   22s   v1.32.2
kind-worker          NotReady   <none>          12s   v1.32.2
kind-worker2         NotReady   <none>          12s   v1.32.2


-----------------------------------------------

cat without-node-selector-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-selector-pod
spec:
  containers:
  - name: nginx-container
    image: nginx


[root@client1 ahmad]# kubectl apply -f without-node-selector-pod.yaml
pod/node-selector-pod created


[root@client1 ahmad]# kubectl get po -o wide
NAME                READY   STATUS              RESTARTS   AGE   IP       NODE          NOMINATED NODE   READINESS GATES
node-selector-pod   0/1     ContainerCreating   0          10s   <none>   kind-worker   <none>           <none>



(It is scheduled on kind-worker randomly)
-----------------------------------------------

vim node-selector-pod.yaml
[root@client1 ahmad]# cat node-selector-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-selector-pod-2
spec:
  containers:
  - name: nginx-container
    image: nginx
  nodeSelector:
    cpu: large


kubectl label node kind-worker2 cpu=large
node/kind-worker2 labeled


-----------------------------------------------

kubectl describe node kind-worker2
Name:               kind-worker2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    cpu=large
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=kind-worker2
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 08 May 2025 17:00:50 +0500
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  kind-worker2
  AcquireTime:     <unset>
  RenewTime:       Thu, 08 May 2025 17:13:46 +0500
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 08 May 2025 17:10:11 +0500   Thu, 08 May 2025 17:00:50 +0500   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 08 May 2025 17:10:11 +0500   Thu, 08 May 2025 17:00:50 +0500   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 08 May 2025 17:10:11 +0500   Thu, 08 May 2025 17:00:50 +0500   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 08 May 2025 17:10:11 +0500   Thu, 08 May 2025 17:01:06 +0500   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.25.0.4
  Hostname:    kind-worker2
Capacity:
  cpu:                2
  ephemeral-storage:  40892Mi
  hugepages-2Mi:      0
  memory:             3745908Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  40892Mi
  hugepages-2Mi:      0
  memory:             3745908Ki
  pods:               110
System Info:
  Machine ID:                 4f9a0002c0914050b1732d0f470ad860
  System UUID:                98b30b5f-0071-4666-93b9-277bf26f458e
  Boot ID:                    9550b1ce-255b-485d-bb69-2a7c8f7556c6
  Kernel Version:             5.14.0-542.el9.x86_64
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://2.0.2
  Kubelet Version:            v1.32.2
  Kube-Proxy Version:         v1.32.2
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
ProviderID:                   kind://docker/kind/kind-worker2
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-svk5b       100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      13m
  kube-system                 kube-proxy-j99tw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (5%)  100m (5%)
  memory             50Mi (1%)  50Mi (1%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 12m                kube-proxy
  Normal  Starting                 13m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  13m (x2 over 13m)  kubelet          Node kind-worker2 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    13m (x2 over 13m)  kubelet          Node kind-worker2 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     13m (x2 over 13m)  kubelet          Node kind-worker2 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  13m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           13m                node-controller  Node kind-worker2 event: Registered Node kind-worker2 in Controller
  Normal  NodeReady                12m                kubelet          Node kind-worker2 status is now: NodeReady




(This node is labelled with cpu=large label now the node-selector-pod-2 will be scheduled on this node specifically)

-----------------------------------------------

kubectl apply -f node-selector-pod.yaml
pod/node-selector-pod-2 created
[root@client1 ahmad]# kubectl get po -o wide
NAME                  READY   STATUS              RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
node-selector-pod     1/1     Running             0          27m   10.244.1.2   kind-worker    <none>           <none>
node-selector-pod-2   0/1     ContainerCreating   0          11s   <none>       kind-worker2   <none>           <none>


(Exactly what we wanted - node-selector-pod-2 is scheduled on kind-worker2)


-----------------------------------------------






